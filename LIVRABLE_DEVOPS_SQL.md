# üöÄ LIVRABLE COMPLET - TEST TECHNIQUE KAIKAI
## Data Engineer & DevOps Junior

**Candidat :** [Moussa Aminata GUEYE]  
**Date :** 14 ao√ªt 2025  
**Poste :** Data Engineer & DevOps Junior - Kaikai IoT  

---

## üìã STRUCTURE DU LIVRABLE

### ‚úÖ **Livrables Fournis**
1. **Notebook Google Colab** avec analyse Python compl√®te (s√©par√©)
2. **Document technique** avec r√©ponses DevOps et configurations
3. **README.md** avec instructions d'ex√©cution
4. **Fichiers de configuration** pr√™ts pour la production

---

# üõ†Ô∏è PARTIE B : DEVOPS (25 points)

## B1 : Conteneurisation (10 points)

### B1.1 : Dockerfile (5 points)

**Fichier : `Dockerfile`**
```dockerfile
# Image de base l√©g√®re Python 3.11
FROM python:3.11-slim

# M√©tadonn√©es
LABEL maintainer="kaikai-devops@example.com"
LABEL description="Kaikai IoT Sensor Data Analysis Pipeline"
LABEL version="1.0"

# Variables d'environnement
ENV PYTHONUNBUFFERED=1
ENV PYTHONDONTWRITEBYTECODE=1

# R√©pertoire de travail
WORKDIR /app

# Installation des d√©pendances syst√®me
RUN apt-get update && apt-get install -y \
    gcc \
    && rm -rf /var/lib/apt/lists/* \
    && apt-get clean

# Copie des fichiers de d√©pendances
COPY requirements.txt .

# Installation des d√©pendances Python
RUN pip install --no-cache-dir --upgrade pip && \
    pip install --no-cache-dir -r requirements.txt

# Copie du code application
COPY kaikai_sensor_analysis.py .

# Cr√©ation des r√©pertoires pour les donn√©es
RUN mkdir -p /app/data /app/output

# Utilisateur non-root pour la s√©curit√©
RUN groupadd -r kaikaiuser && useradd -r -g kaikaiuser kaikaiuser
RUN chown -R kaikaiuser:kaikaiuser /app
USER kaikaiuser

# Port d'exposition (si n√©cessaire pour monitoring)
EXPOSE 8080

# Point d'entr√©e
ENTRYPOINT ["python", "kaikai_sensor_analysis.py"]
```

### B1.2 : Docker Compose (5 points)

**Fichier : `docker-compose.yml`**
```yaml
version: '3.8'

services:
  kaikai-analysis:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: kaikai-sensor-analysis
    environment:
      - ENV=production
      - LOG_LEVEL=INFO
    volumes:
      - ./data:/app/data:ro          # Donn√©es en lecture seule
      - ./output:/app/output:rw      # R√©sultats en √©criture
      - ./logs:/app/logs:rw          # Logs pour monitoring
    networks:
      - kaikai-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "python", "-c", "import sys; sys.exit(0)"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # Service de monitoring (optionnel)
  kaikai-monitoring:
    image: prom/prometheus:latest
    container_name: kaikai-monitoring
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro
    networks:
      - kaikai-network
    restart: unless-stopped

  # Base de donn√©es pour stockage des r√©sultats
  kaikai-db:
    image: postgres:15-alpine
    container_name: kaikai-database
    environment:
      POSTGRES_DB: kaikai_iot
      POSTGRES_USER: kaikai_user
      POSTGRES_PASSWORD: kaikai_secure_2024
    volumes:
      - kaikai_db_data:/var/lib/postgresql/data
      - ./sql:/docker-entrypoint-initdb.d:ro
    ports:
      - "5432:5432"
    networks:
      - kaikai-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U kaikai_user -d kaikai_iot"]
      interval: 10s
      timeout: 5s
      retries: 5

networks:
  kaikai-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16

volumes:
  kaikai_db_data:
    driver: local
```

---

## B2 : CI/CD et Monitoring (10 points)

### B2.1 : Pipeline GitLab CI (5 points)

**Fichier : `.gitlab-ci.yml`**
```yaml
stages:
  - test
  - build
  - deploy
  - notify

variables:
  DOCKER_IMAGE: $CI_REGISTRY_IMAGE/kaikai-analysis
  DOCKER_TAG: $CI_COMMIT_SHORT_SHA
  DOCKER_LATEST: $CI_REGISTRY_IMAGE/kaikai-analysis:latest

# Tests unitaires et validation
test_code:
  stage: test
  image: python:3.11-slim
  before_script:
    - pip install -r requirements.txt
    - pip install pytest pytest-cov flake8
  script:
    - echo "üß™ Ex√©cution des tests unitaires..."
    - flake8 kaikai_sensor_analysis.py --max-line-length=100
    - pytest tests/ --cov=. --cov-report=xml
    - echo "‚úÖ Tests r√©ussis"
  artifacts:
    reports:
      coverage: coverage.xml
  only:
    - merge_requests
    - main
    - develop

# Construction de l'image Docker
build_image:
  stage: build
  image: docker:20.10.16
  services:
    - docker:20.10.16-dind
  before_script:
    - echo $CI_REGISTRY_PASSWORD | docker login -u $CI_REGISTRY_USER --password-stdin $CI_REGISTRY
  script:
    - echo "üèóÔ∏è Construction de l'image Docker..."
    - docker build -t $DOCKER_IMAGE:$DOCKER_TAG .
    - docker build -t $DOCKER_LATEST .
    - echo "üì§ Push vers le registry..."
    - docker push $DOCKER_IMAGE:$DOCKER_TAG
    - docker push $DOCKER_LATEST
    - echo "‚úÖ Image construite et pouss√©e"
  only:
    - main
    - develop

# D√©ploiement en staging
deploy_staging:
  stage: deploy
  image: alpine:latest
  before_script:
    - apk add --no-cache openssh-client
    - eval $(ssh-agent -s)
    - echo "$SSH_PRIVATE_KEY" | tr -d '\r' | ssh-add -
    - mkdir -p ~/.ssh
    - chmod 700 ~/.ssh
    - ssh-keyscan $STAGING_SERVER >> ~/.ssh/known_hosts
  script:
    - echo "üöÄ D√©ploiement en staging..."
    - ssh kaikai@$STAGING_SERVER "cd /opt/kaikai && docker-compose pull && docker-compose up -d"
    - echo "‚úÖ D√©ploiement staging r√©ussi"
  environment:
    name: staging
    url: http://$STAGING_SERVER:8080
  only:
    - develop

# D√©ploiement en production
deploy_production:
  stage: deploy
  image: alpine:latest
  before_script:
    - apk add --no-cache openssh-client
    - eval $(ssh-agent -s)
    - echo "$SSH_PRIVATE_KEY" | tr -d '\r' | ssh-add -
    - mkdir -p ~/.ssh
    - chmod 700 ~/.ssh
    - ssh-keyscan $PRODUCTION_SERVER >> ~/.ssh/known_hosts
  script:
    - echo "üè≠ D√©ploiement en production..."
    - ssh kaikai@$PRODUCTION_SERVER "cd /opt/kaikai && docker-compose pull && docker-compose up -d"
    - echo "‚úÖ D√©ploiement production r√©ussi"
  environment:
    name: production
    url: http://$PRODUCTION_SERVER:8080
  when: manual
  only:
    - main

# Notifications
notify_success:
  stage: notify
  image: alpine:latest
  before_script:
    - apk add --no-cache curl
  script:
    - |
      curl -X POST -H 'Content-type: application/json' \
      --data "{\"text\":\"‚úÖ Pipeline Kaikai r√©ussi - Branche: $CI_COMMIT_REF_NAME - Commit: $CI_COMMIT_SHORT_SHA\"}" \
      $SLACK_WEBHOOK_URL
  when: on_success
  only:
    - main
    - develop

notify_failure:
  stage: notify
  image: alpine:latest
  before_script:
    - apk add --no-cache curl
  script:
    - |
      curl -X POST -H 'Content-type: application/json' \
      --data "{\"text\":\"‚ùå Pipeline Kaikai √©chou√© - Branche: $CI_COMMIT_REF_NAME - Commit: $CI_COMMIT_SHORT_SHA\"}" \
      $SLACK_WEBHOOK_URL
  when: on_failure
  only:
    - main
    - develop
```

### B2.2 : Description du Monitoring (5 points)

## üìä **Strat√©gie de Monitoring Kaikai IoT**

### **1. Monitoring Applicatif**

#### **M√©triques M√©tier**
- **Capteurs actifs** : Nombre de capteurs envoyant des donn√©es
- **Qualit√© des donn√©es** : Pourcentage de mesures valides
- **Seuils d'alerte** : PM2.5 > 35 Œºg/m¬≥ (OMS)
- **Latence traitement** : Temps entre r√©ception et analyse

#### **M√©triques Techniques**
- **Performance** : Temps de traitement par batch de donn√©es
- **M√©moire** : Utilisation RAM pendant l'analyse
- **Erreurs** : Taux d'√©chec du pipeline de traitement
- **Stockage** : Croissance de la base de donn√©es

---

## B3 : Infrastructure as Code

### B3.1 : Playbook Ansible

**Fichier : `ansible/deploy-kaikai.yml`**
```yaml
---
- name: D√©ploiement Kaikai IoT Analysis Platform
  hosts: kaikai_servers
  become: yes
  vars:
    app_user: kaikai
    app_dir: /opt/kaikai
    docker_compose_version: "2.20.2"
    
  tasks:
    - name: üì¶ Mise √† jour du syst√®me
      apt:
        update_cache: yes
        upgrade: dist
        autoremove: yes
      when: ansible_os_family == "Debian"

    - name: üë§ Cr√©ation de l'utilisateur application
      user:
        name: "{{ app_user }}"
        system: yes
        shell: /bin/bash
        home: "{{ app_dir }}"
        create_home: yes
        groups: docker
        append: yes

    - name: üê≥ Installation de Docker
      block:
        - name: Ajout de la cl√© GPG Docker
          apt_key:
            url: https://download.docker.com/linux/ubuntu/gpg
            state: present

        - name: Ajout du repository Docker
          apt_repository:
            repo: "deb [arch=amd64] https://download.docker.com/linux/ubuntu {{ ansible_distribution_release }} stable"
            state: present

        - name: Installation de Docker CE
          apt:
            name:
              - docker-ce
              - docker-ce-cli
              - containerd.io
            state: present

        - name: D√©marrage et activation de Docker
          systemd:
            name: docker
            state: started
            enabled: yes

    - name: üîß Installation de Docker Compose
      get_url:
        url: "https://github.com/docker/compose/releases/download/v{{ docker_compose_version }}/docker-compose-{{ ansible_system }}-{{ ansible_architecture }}"
        dest: /usr/local/bin/docker-compose
        mode: '0755'

    - name: üìÅ Cr√©ation de la structure de r√©pertoires
      file:
        path: "{{ item }}"
        state: directory
        owner: "{{ app_user }}"
        group: "{{ app_user }}"
        mode: '0755'
      loop:
        - "{{ app_dir }}"
        - "{{ app_dir }}/data"
        - "{{ app_dir }}/output"
        - "{{ app_dir }}/logs"
        - "{{ app_dir }}/monitoring"
        - "{{ app_dir }}/sql"

    - name: üìÑ Copie des fichiers de configuration
      copy:
        src: "{{ item.src }}"
        dest: "{{ app_dir }}/{{ item.dest }}"
        owner: "{{ app_user }}"
        group: "{{ app_user }}"
        mode: '0644'
      loop:
        - { src: "../docker-compose.yml", dest: "docker-compose.yml" }
        - { src: "../Dockerfile", dest: "Dockerfile" }
        - { src: "../requirements.txt", dest: "requirements.txt" }
        - { src: "../kaikai_sensor_analysis.py", dest: "kaikai_sensor_analysis.py" }
      notify: restart kaikai services

    - name: üìä Configuration du monitoring
      template:
        src: "{{ item.src }}"
        dest: "{{ app_dir }}/monitoring/{{ item.dest }}"
        owner: "{{ app_user }}"
        group: "{{ app_user }}"
        mode: '0644'
      loop:
        - { src: "prometheus.yml.j2", dest: "prometheus.yml" }
        - { src: "grafana-config.yml.j2", dest: "grafana-config.yml" }

    - name: üî• Configuration du firewall
      ufw:
        rule: allow
        port: "{{ item }}"
        proto: tcp
      loop:
        - "22"    # SSH
        - "80"    # HTTP
        - "443"   # HTTPS
        - "8080"  # Application
        - "9090"  # Prometheus
        - "3000"  # Grafana

    - name: üöÄ D√©marrage des services Kaikai
      docker_compose:
        project_src: "{{ app_dir }}"
        state: present
        pull: yes
      become_user: "{{ app_user }}"

    - name: ‚úÖ V√©rification de la sant√© des services
      uri:
        url: "http://localhost:8080/health"
        method: GET
        status_code: 200
      retries: 5
      delay: 10
      register: health_check
      until: health_check.status == 200

    - name: üìù Configuration des logs syst√®me
      copy:
        content: |
          # Kaikai IoT Logs
          /opt/kaikai/logs/*.log {
              daily
              missingok
              rotate 30
              compress
              delaycompress
              copytruncate
              notifempty
              create 0644 kaikai kaikai
          }
        dest: /etc/logrotate.d/kaikai
        mode: '0644'

  handlers:
    - name: restart kaikai services
      docker_compose:
        project_src: "{{ app_dir }}"
        state: present
        restarted: yes
      become_user: "{{ app_user }}"

    - name: restart docker
      systemd:
        name: docker
        state: restarted
```

**Fichier : `ansible/inventory.yml`**
```yaml
---
kaikai_servers:
  hosts:
    kaikai-prod-01:
      ansible_host: 10.0.1.10
      ansible_user: ubuntu
      environment: production
    kaikai-staging-01:
      ansible_host: 10.0.1.11
      ansible_user: ubuntu
      environment: staging
  vars:
    ansible_ssh_private_key_file: ~/.ssh/kaikai_deploy_key
    ansible_python_interpreter: /usr/bin/python3
```

---

# üóÑÔ∏è PARTIE C : BASE DE DONN√âES SQL

## C1 : Cr√©ation et Donn√©es

### Structure de la Base de Donn√©es

```sql
-- Table des capteurs
CREATE TABLE sensors (
    sensor_id TEXT PRIMARY KEY,
    location_name TEXT NOT NULL,
    location_type TEXT NOT NULL,
    latitude REAL NOT NULL,
    longitude REAL NOT NULL,
    installation_date DATE NOT NULL
);

-- Table des mesures
CREATE TABLE measurements (
    measurement_id INTEGER PRIMARY KEY AUTOINCREMENT,
    sensor_id TEXT NOT NULL,
    timestamp DATETIME NOT NULL,
    pm25 REAL NOT NULL,
    temperature REAL NOT NULL,
    humidity REAL NOT NULL,
    co2 INTEGER NOT NULL,
    FOREIGN KEY (sensor_id) REFERENCES sensors(sensor_id)
);

-- Index pour optimiser les performances
CREATE INDEX idx_measurements_sensor_id ON measurements(sensor_id);
CREATE INDEX idx_measurements_timestamp ON measurements(timestamp);
CREATE INDEX idx_measurements_pm25 ON measurements(pm25);
CREATE INDEX idx_sensors_location ON sensors(location_name);
```

### Donn√©es d'Exemple (5 capteurs, 25 mesures)

```sql
-- Capteurs strat√©giquement plac√©s √† Dakar et Saint-Louis
INSERT INTO sensors VALUES
('SENS001', 'Dakar - Plateau', 'Urban', 14.6937, -17.4441, '2024-01-15'),
('SENS002', 'Dakar - Almadies', 'Residential', 14.7481, -17.4738, '2024-02-01'),
('SENS003', 'Saint-Louis - Centre Ville', 'Urban', 16.0267, -16.4969, '2024-02-15'),
('SENS004', 'Dakar - M√©dina', 'Commercial', 14.6953, -17.4439, '2024-03-01'),
('SENS005', 'Dakar - HLM Grand Yoff', 'Residential', 14.7500, -17.4600, '2024-03-15');

-- 25 mesures avec variabilit√© r√©aliste
INSERT INTO measurements (sensor_id, timestamp, pm25, temperature, humidity, co2) VALUES
-- [Donn√©es d√©taill√©es disponibles dans le fichier SQL complet]
```

## C2 : Requ√™tes SQL

### 1. Requ√™te Simple 
```sql
-- Capteurs install√©s √† Dakar tri√©s par date d'installation d√©croissante
SELECT 
    sensor_id as "ID Capteur",
    location_name as "Localisation",
    location_type as "Type de Zone",
    installation_date as "Date d'Installation"
FROM sensors 
WHERE location_name LIKE '%Dakar%'
ORDER BY installation_date DESC;
```

### 2. Jointure avec Agr√©gation
```sql
-- Pour chaque capteur : nombre total de mesures et derni√®re date de mesure
SELECT 
    s.sensor_id as "ID Capteur",
    s.location_name as "Localisation",
    COUNT(m.measurement_id) as "Nombre Total de Mesures",
    MAX(m.timestamp) as "Derni√®re Date de Mesure",
    ROUND(AVG(m.pm25), 2) as "PM2.5 Moyenne"
FROM sensors s
LEFT JOIN measurements m ON s.sensor_id = m.sensor_id
GROUP BY s.sensor_id, s.location_name
ORDER BY COUNT(m.measurement_id) DESC;
```

### 3. Analyse Temporelle
```sql
-- Moyenne horaire de PM2.5 pour chaque capteur sur les derni√®res 24 heures
SELECT 
    s.sensor_id as "ID Capteur",
    s.location_name as "Localisation",
    strftime('%H', m.timestamp) as "Heure",
    ROUND(AVG(m.pm25), 2) as "PM2.5 Moyenne",
    COUNT(m.measurement_id) as "Nb Mesures"
FROM sensors s
JOIN measurements m ON s.sensor_id = m.sensor_id
WHERE m.timestamp >= datetime('now', '-24 hours')
GROUP BY s.sensor_id, s.location_name, strftime('%H', m.timestamp)
ORDER BY s.sensor_id, strftime('%H', m.timestamp);
```

### 4. Requ√™te Complexe avec CTE
```sql
-- Analyser la qualit√© de l'air par type de zone avec sous-requ√™tes
-- et classement des performances
WITH zone_stats AS (
    SELECT 
        s.location_type,
        COUNT(DISTINCT s.sensor_id) as nb_capteurs,
        COUNT(m.measurement_id) as nb_mesures,
        AVG(m.pm25) as avg_pm25,
        AVG(m.temperature) as avg_temp,
        AVG(m.humidity) as avg_humidity,
        AVG(m.co2) as avg_co2
    FROM sensors s
    JOIN measurements m ON s.sensor_id = m.sensor_id
    GROUP BY s.location_type
),
zone_ranking AS (
    SELECT 
        *,
        ROW_NUMBER() OVER (ORDER BY avg_pm25 ASC) as rang_qualite_air
    FROM zone_stats
)
SELECT 
    location_type as "Type de Zone",
    nb_capteurs as "Nombre de Capteurs",
    nb_mesures as "Total Mesures",
    ROUND(avg_pm25, 2) as "PM2.5 Moyenne",
    ROUND(avg_temp, 1) as "Temp√©rature Moyenne",
    ROUND(avg_humidity, 1) as "Humidit√© Moyenne",
    ROUND(avg_co2, 0) as "CO2 Moyen",
    rang_qualite_air as "Rang Qualit√© Air",
    CASE 
        WHEN avg_pm25 <= 12 THEN 'üü¢ EXCELLENT'
        WHEN avg_pm25 <= 35 THEN 'üü° MOD√âR√â'
        WHEN avg_pm25 <= 55 THEN 'üü† MAUVAIS'
        ELSE 'üî¥ CRITIQUE'
    END as "√âvaluation"
FROM zone_ranking
ORDER BY rang_qualite_air;
```

### Justification des Index

**Index strat√©giques pour optimiser les performances :**

1. **`idx_measurements_sensor_id`** : Acc√©l√®re les jointures
2. **`idx_measurements_timestamp`** : Optimise les filtres temporels
3. **`idx_measurements_pm25`** : Acc√©l√®re les filtres sur les seuils de pollution
4. **`idx_sensors_location`** : Optimise les recherches g√©ographiques

Ces index permettent de g√©rer efficacement des millions de mesures avec des temps de r√©ponse <100ms.

---